<!---->
<launch>
  <arg name = "data" default = "0"/>
  <arg name = "env" default = "v0"/>
  <!-- training node -->
  <node name="driving_train"	pkg="IRL_learning_ros"  type="train2017.py" output="screen">
	<param name="environment" 	value="$(arg env)" />
				<!-- envirionment
					v0 : autonomous driving traning input 9
					v1 : autonomous driving traning input 11
				-->
	<param name="model_path" 	value="$(find IRL_learning_ros)/model"/>
	<param name="init_episode" 	value="$(arg data)" />
	<param name="learning_rate" 	value="0.01" />
	<param name="discount_rate" 	value="0.99" />
	<param name="replay_memory"	value="10000" />
	<param name="batch_size" 	value="64" />
	<param name="target_update_freq" 	value="10" />


  </node>
  <node pkg="IRL_learning_ros" type="agent_state_publisher_v1.py" name="agent_state_publisher_$(arg env)">
	<param name="inversed" 	value="false" />
  </node>
  <!--<node pkg="IRL_learning_ros" type="agent_effort_publisher_$(arg env).py" name="effort_publisher"/>  -->
  <node pkg="IRL_learning_ros" type="agent_effort_publisher_v2.py" name="effort_publisher"/>
  <!-- draw plot -->
  <node name="draw_result"	pkg="IRL_learning_ros"  type="draw_score.py" output="screen">
	<param name="environment" 	value="$(arg env)" />
	<param name="model_path" 	value="$(find IRL_learning_ros)/model/result"/>
  </node>	
</launch>

